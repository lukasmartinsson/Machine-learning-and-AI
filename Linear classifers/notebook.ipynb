{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Programming Assignment 4: Linear Classifers\nGroup 40: Albin Jansfelt, Amanda Allgurén, Lukas Martinsson\n\n## Exercise Questions\n\n**Why could the classifier \"memorize\" the training data in the first case, but not in the second case?**\n\nThe Perceptron and LinearSVC tries to linearly separate the groups in training. In the second case, all combinations have different output labels. Meaning *(Sydney, July) -> rain* and *(Sydney, December) -> sun*. In the first case *(Gothenburg,July) -> rain* and *(Gothenburg, December) -> rain*. This makes it linearly separable. The model has not memorized the training data, but found a linear function to separate it as best as possible.\n\nThe second case can be compared to an XOR gate. We have the four data points in four corners where the same label (sun/rain) is diagonal to eachother, meaning it is impossible to linearly separate them.\n\nThe change of city names from Gothenburg to Sydney does not affect the outcome of the classifiers.",
   "metadata": {
    "cell_id": "63d46101-6750-45c1-b95d-219e4e40375b",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Results:\nPerceptron: Training time: 3.39 sec, Accuracy: 0.7919 \\\nPegasos algorithm SVC: Training time: 5.33 sec, Accuracy: 0.8326 \\\nPegasos algorithm Logistic: Training time: 4.40 sec, Accuracy: 0.8317 \\\na) Faster linear algebra operations: Training time: 2.62 sec, Accuracy: 0.8326 \\\nb) Using sparse vectors: Training time: 4.09 sec, Accuracy: 0.8326 \\\nc) Speeding up the scaling operation: Training time: 3.89 sec, Accuracy: 0.8338",
   "metadata": {
    "cell_id": "a16e3c2e-6f3b-4f49-b5d9-8a7b5e047e17",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "247e848a-6549-42ef-a1ce-3fe7c0d118ae",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af6f511c",
    "execution_start": 1645733946874,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "import time\nimport numpy as np\nimport sys\nimport scipy.linalg.blas as blas\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom aml_perceptron import Perceptron, SparsePerceptron, LinearClassifier",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Original Perceptron",
   "metadata": {
    "cell_id": "456c5186-0932-45ee-8ad4-54cd511882e1",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "7da972ea-117f-460b-b9c7-8aa40aa0988c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "881569a2",
    "execution_start": 1645733231220,
    "execution_millis": 3768,
    "deepnote_cell_type": "code"
   },
   "source": "# This function reads the corpus, returns a list of documents, and a list\n# of their corresponding polarity labels. \ndef read_data(corpus_file):\n    X = []\n    Y = []\n    with open(corpus_file, encoding='utf-8') as f:\n        for line in f:\n            _, y, _, x = line.split(maxsplit=3)\n            X.append(x.strip())\n            Y.append(y)\n    return X, Y\n\nif __name__ == '__main__':\n    \n    # Read all the documents.\n    X, Y = read_data('data/all_sentiment_shuffled.txt')\n    \n    # Split into training and test parts.\n    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n                                                    random_state=0)\n\n    # Set up the preprocessing steps and the classifier.\n    pipeline = make_pipeline(\n        TfidfVectorizer(),\n        SelectKBest(k=1000),\n        Normalizer(),\n\n        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n        Perceptron()  \n    )\n\n    # Train the classifier.\n    t0 = time.time()\n    pipeline.fit(Xtrain, Ytrain)\n    t1 = time.time()\n    print('Training time: {:.2f} sec.'.format(t1-t0))\n\n    # Evaluate on the test set.\n    Yguess = pipeline.predict(Xtest)\n    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 3.39 sec.\nAccuracy: 0.7919.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pegasos algorithm: SVC",
   "metadata": {
    "cell_id": "9c56a526-ee4b-4df5-87bc-73d00ecc5f4d",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6f5f82cb-34b7-410c-b89f-594eaab3f372",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ca61bed8",
    "execution_start": 1645733436218,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "class Pegasos(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=10, lam=1/10000):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n        self.lam = lam\n\n    #lamda parameter added\n    def fit(self, X, Y):\n        \"\"\"\n        Train a linear classifier using the perceptron learning algorithm.\n        \"\"\"\n        \n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        self.find_classes(Y)\n\n        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n        Ye = self.encode_outputs(Y)\n\n        # If necessary, convert the sparse matrix returned by a vectorizer\n        # into a normal NumPy matrix.\n        if not isinstance(X, np.ndarray):\n            X = X.toarray()\n\n        # Initialize the weight vector to all zeros.\n        n_features = X.shape[1]\n        self.w = np.zeros(n_features)\n\n        # Perceptron algorithm:\n        '''\n        for i in range(self.n_iter):\n            for x, y in zip(X, Ye):\n\n                # Compute the output score for this instance.\n                score = x.dot(self.w)\n\n                # If there was an error, update the weights.\n                if y*score <= 0:\n                    self.w += y*x\n        '''\n\n        #Pegasos algorithm, SVC\n        t=0\n        for i in range(self.n_iter):\n            for x,y in zip(X, Ye):\n                t+=1\n                eta = 1 / (self.lam*t)\n                score = np.dot(self.w, x)\n                if(y*score < 1):\n                    self.w = (1-eta*self.lam)*self.w + (eta*y)*x\n                else:\n                    self.w *= (1-eta*self.lam)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e4a418fa-2085-4a9f-8004-80e5676b009e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "66b5e92f",
    "execution_start": 1645733438042,
    "execution_millis": 6028,
    "deepnote_cell_type": "code"
   },
   "source": "# This function reads the corpus, returns a list of documents, and a list\n# of their corresponding polarity labels. \ndef read_data(corpus_file):\n    X = []\n    Y = []\n    with open(corpus_file, encoding='utf-8') as f:\n        for line in f:\n            _, y, _, x = line.split(maxsplit=3)\n            X.append(x.strip())\n            Y.append(y)\n    return X, Y\n\n\nif __name__ == '__main__':\n    \n    # Read all the documents.\n    X, Y = read_data('data/all_sentiment_shuffled.txt')\n    \n    # Split into training and test parts.\n    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n                                                    random_state=0)\n\n    # Set up the preprocessing steps and the classifier.\n    pipeline = make_pipeline(\n        TfidfVectorizer(),\n        SelectKBest(k=1000),\n        Normalizer(),\n        Pegasos(10, 1/len(Xtrain))  \n    )\n    alg = 1\n    # Train the classifier.\n    t0 = time.time()\n    pipeline.fit(Xtrain, Ytrain)\n    t1 = time.time()\n    print('Training time: {:.2f} sec.'.format(t1-t0))\n\n    # Evaluate on the test set.\n    Yguess = pipeline.predict(Xtest)\n    print('Accuracy Pegasos algorithm SVC: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 5.33 sec.\nAccuracy Pegasos algorithm SVC: 0.8326.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Pegasos algorithm: Logistic",
   "metadata": {
    "cell_id": "e00f11c5-2c0f-41b2-809a-7df9d9da04c3",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e4eb502d-ce45-4c74-a538-b79bddece9c7",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d565eca9",
    "execution_start": 1645733527582,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "class Pegasos(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=10, lam=1/10000):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n        self.lam = lam\n\n    #lamda parameter added\n    def fit(self, X, Y):\n        \"\"\"\n        Train a linear classifier using the perceptron learning algorithm.\n        \"\"\"\n        \n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        self.find_classes(Y)\n\n        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n        Ye = self.encode_outputs(Y)\n\n        # If necessary, convert the sparse matrix returned by a vectorizer\n        # into a normal NumPy matrix.\n        if not isinstance(X, np.ndarray):\n            X = X.toarray()\n\n        # Initialize the weight vector to all zeros.\n        n_features = X.shape[1]\n        self.w = np.zeros(n_features)\n\n        #Pegasos algorithm, Logistic\n        t=0\n        for i in range(self.n_iter):\n            for x,y in zip(X, Ye):\n                t += 1\n                eta = 1 / (self.lam*t)\n                score = np.dot(self.w, x)\n                gradf = self.lam*self.w - np.divide(y, (1+np.exp(y*score)))*x\n                self.w -= eta*gradf\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "61a65f32-69d4-40d5-bd90-3d1397a4be57",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "491369c6",
    "execution_start": 1645733923377,
    "execution_millis": 4788,
    "deepnote_cell_type": "code"
   },
   "source": "# This function reads the corpus, returns a list of documents, and a list\n# of their corresponding polarity labels. \ndef read_data(corpus_file):\n    X = []\n    Y = []\n    with open(corpus_file, encoding='utf-8') as f:\n        for line in f:\n            _, y, _, x = line.split(maxsplit=3)\n            X.append(x.strip())\n            Y.append(y)\n    return X, Y\n\n\nif __name__ == '__main__':\n    \n    # Read all the documents.\n    X, Y = read_data('data/all_sentiment_shuffled.txt')\n    \n    # Split into training and test parts.\n    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n                                                    random_state=0)\n\n    # Set up the preprocessing steps and the classifier.\n    pipeline = make_pipeline(\n        TfidfVectorizer(),\n        SelectKBest(k=1000),\n        Normalizer(),\n        Pegasos(10, 1/len(Xtrain))  \n    )\n    alg = 1\n    # Train the classifier.\n    t0 = time.time()\n    pipeline.fit(Xtrain, Ytrain)\n    t1 = time.time()\n    print('Training time: {:.2f} sec.'.format(t1-t0))\n\n    # Evaluate on the test set.\n    Yguess = pipeline.predict(Xtest)\n    print('Accuracy Pegasos algorithm Logistic: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 4.40 sec.\nAccuracy Pegasos algorithm Logistic: 0.8317.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Bonus task 1\n#### a) Faster linear algebra operations",
   "metadata": {
    "cell_id": "433d93fe-d1db-480e-bbfc-5287e1a17eb8",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e5029012-93a7-421d-b226-f982f375c654",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dd1227a0",
    "execution_start": 1645734048557,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "class PegasosImproved(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=10, lam=1/10000):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n        self.lam = lam\n\n    #lamda parameter added\n    def fit(self, X, Y):\n        \"\"\"\n        Train a linear classifier using the perceptron learning algorithm.\n        \"\"\"\n        \n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        self.find_classes(Y)\n\n        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n        Ye = self.encode_outputs(Y)\n\n        # If necessary, convert the sparse matrix returned by a vectorizer\n        # into a normal NumPy matrix.\n        if not isinstance(X, np.ndarray):\n            X = X.toarray()\n\n        # Initialize the weight vector to all zeros.\n        n_features = X.shape[1]\n        self.w = np.zeros(n_features)\n\n        #Pegasos algorithm improved, SVC\n        t=0\n        for i in range(self.n_iter):\n            for x,y in zip(X, Ye):\n                t+=1\n                eta = 1 / (self.lam*t)\n                score = blas.ddot(self.w, x)\n            \n                blas.dscal((1-eta*self.lam), self.w)\n                if(y*score < 1):\n                    blas.daxpy(x,self.w,a=eta*y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "89bd398a-ecf0-4831-8e5f-fc57f1dbe055",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f623cdd1",
    "execution_start": 1645734050502,
    "execution_millis": 2900,
    "deepnote_cell_type": "code"
   },
   "source": "\n# Set up the preprocessing steps and the classifier.\npipeline = make_pipeline(\n    TfidfVectorizer(),\n    SelectKBest(k=1000),\n    Normalizer(),\n    #bestäm rimligt lambda\n    PegasosImproved(10, 1/len(Xtrain))  \n)\n\n\n# Train the classifier.\nt0 = time.time()\npipeline.fit(Xtrain, Ytrain)\nt1 = time.time()\nprint('Training time: {:.2f} sec.'.format(t1-t0))\n\n# Evaluate on the test set.\nYguess = pipeline.predict(Xtest)\nprint('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 2.62 sec.\nAccuracy: 0.8326.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### b) Using sparse vectors",
   "metadata": {
    "cell_id": "0be1a7b0-4b3b-4d2d-a728-08d984a71976",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "64395d12-e64f-46db-96a7-f6b426d68b90",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ce81bb07",
    "execution_start": 1645734089087,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "def add_sparse_to_dense(x, w, factor):\n    \"\"\"\n    Adds a sparse vector x, scaled by some factor, to a dense vector.\n    This can be seen as the equivalent of w += factor * x when x is a dense\n    vector.\n    \"\"\"\n    w[x.indices] += factor * x.data\n    return w\n\ndef sparse_dense_dot(x, w):\n    \"\"\"\n    Computes the dot product between a sparse vector x and a dense vector w.\n    \"\"\"\n    return np.dot(w[x.indices], x.data)\n\nclass SparseSVC(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=10, lam=1/10000):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n        self.lam = lam\n\n    #lamda parameter added\n    def fit(self, X, Y):\n\n        self.find_classes(Y)\n\n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        Ye = self.encode_outputs(Y)\n\n        # Initialize the weight vector to all zeros.\n        #print(X.shape[1])\n        self.w = np.zeros(X.shape[1])\n\n        # Iteration through sparse matrices can be a bit slow, so we first\n        # prepare this list to speed up iteration.\n        XY = list(zip(X, Ye))\n        \n        #Pegasos algorithm, SVC \n   \n        t=0\n        for i in range(self.n_iter):\n            for x,y in XY:\n                t+=1\n                eta = 1 / (self.lam*t)\n                score = sparse_dense_dot(x, self.w)\n                self.w *= (1-eta*self.lam)\n             \n                if(y*score < 1):\n                    self.w = add_sparse_to_dense(x,self.w,(eta*y))\n                    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "abb7b7c4-6149-4bd1-8bc5-9400ce73afa0",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "53b969f8",
    "execution_start": 1645734091530,
    "execution_millis": 4607,
    "deepnote_cell_type": "code"
   },
   "source": "# Set up the preprocessing steps and the classifier.\npipeline = make_pipeline(\n    TfidfVectorizer(),\n    SelectKBest(k=1000),\n    Normalizer(),\n    #bestäm rimligt lambda\n    SparseSVC(10, 1/len(Xtrain))  \n)\n\n# Train the classifier.\nt0 = time.time()\npipeline.fit(Xtrain, Ytrain)\nt1 = time.time()\nprint('Training time: {:.2f} sec.'.format(t1-t0))\n\n# Evaluate on the test set.\nYguess = pipeline.predict(Xtest)\nprint('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 4.09 sec.\nAccuracy: 0.8326.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### c) Speeding up the scaling operation",
   "metadata": {
    "cell_id": "d8da87cd-d558-41a7-9e32-3f0321a29e6b",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6b74be72-07b7-493c-877c-adb3fa425682",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "494984a6",
    "execution_start": 1645734126079,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def add_sparse_to_dense(x, w, factor):\n    \"\"\"\n    Adds a sparse vector x, scaled by some factor, to a dense vector.\n    This can be seen as the equivalent of w += factor * x when x is a dense\n    vector.\n    \"\"\"\n    w[x.indices] += factor * x.data\n    return w\n\ndef sparse_dense_dot(x, w):\n    \"\"\"\n    Computes the dot product between a sparse vector x and a dense vector w.\n    \"\"\"\n    return np.dot(w[x.indices], x.data)\n\nclass SparseSVCImproved(LinearClassifier):\n    \"\"\"\n    A straightforward implementation of the perceptron learning algorithm.\n    \"\"\"\n\n    def __init__(self, n_iter=10, lam=1/10000):\n        \"\"\"\n        The constructor can optionally take a parameter n_iter specifying how\n        many times we want to iterate through the training set.\n        \"\"\"\n        self.n_iter = n_iter\n        self.lam = lam\n\n    #lamda parameter added\n    def fit(self, X, Y):\n\n        self.find_classes(Y)\n\n        # First determine which output class will be associated with positive\n        # and negative scores, respectively.\n        Ye = self.encode_outputs(Y)\n\n        # Initialize the weight vector to all zeros.\n        #print(X.shape[1])\n        self.w = np.zeros(X.shape[1])\n\n\n        # Iteration through sparse matrices can be a bit slow, so we first\n        # prepare this list to speed up iteration.\n        XY = list(zip(X, Ye))\n        \n        a=1\n        t=1\n        for i in range(self.n_iter):\n            for x,y in XY:\n                t+=1\n                eta = 1 / (self.lam*t)\n                score = a*sparse_dense_dot(x, self.w)\n                a *= (1-eta*self.lam)\n             \n                if(y*score < 1):\n                    add_sparse_to_dense(x,self.w,(eta*y)/a)\n        self.w *= a           \n                    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "87186850-f2bb-4dc6-abec-ef8d33be4550",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7377c3e6",
    "execution_start": 1645734128285,
    "execution_millis": 4497,
    "deepnote_cell_type": "code"
   },
   "source": "pipeline = make_pipeline(\n    TfidfVectorizer(),\n    SelectKBest(k=1000),\n    Normalizer(),\n    #bestäm rimligt lambda\n    SparseSVCImproved(10, 1/len(Xtrain))  \n)\n\n# Train the classifier.\nt0 = time.time()\npipeline.fit(Xtrain, Ytrain)\nt1 = time.time()\nprint('Training time: {:.2f} sec.'.format(t1-t0))\n\n# Evaluate on the test set.\nYguess = pipeline.predict(Xtest)\nprint('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training time: 3.89 sec.\nAccuracy: 0.8338.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7c023ec2-1210-4861-82d1-6d35a9cb94f6' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "7847f4aa-eeec-479c-8348-262f1f83cfcf",
  "deepnote_execution_queue": []
 }
}