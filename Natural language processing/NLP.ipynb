{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random \n",
        "from operator import itemgetter \n",
        "from collections import Counter, defaultdict\n",
        "import sys\n",
        "\n",
        "dataset = {\"sv\": \"europarl-v7.sv-en.lc.sv\",\n",
        "           \"en\": \"europarl-v7.sv-en.lc.en\"}\n",
        "\n",
        "MIN_PROB = sys.float_info.min"
      ],
      "metadata": {
        "id": "ODGtAJtMvumG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus:\n",
        "  def __init__(self, file_name, encoding):\n",
        "    with open(file_name, encoding=encoding) as f:\n",
        "      all_lines = f.readlines()\n",
        "\n",
        "    self.documents = self.__get_documents(all_lines)\n",
        "    self.word_counts = self.__get_counts(self.documents)\n",
        "\n",
        "  def __get_documents(self, lines: list) -> list:\n",
        "    documents = []\n",
        "    for line in lines:\n",
        "      words = line.split()\n",
        "      documents.append(words)\n",
        "    return documents\n",
        "\n",
        "  def __get_counts(self, documents: list) -> Counter:\n",
        "    counts = Counter()\n",
        "    for i, doc in enumerate(documents):\n",
        "      for word in doc:\n",
        "        counts[word] += 1\n",
        "    return counts\n",
        "\n",
        "  def get_vocab(self) -> list:\n",
        "    return self.word_counts.keys()"
      ],
      "metadata": {
        "id": "P62oG1NbvexL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vocabs and documents, print some example sentences\n",
        "corp_sv = Corpus(file_name=dataset['sv'], encoding=\"UTF-8\")\n",
        "corp_en = Corpus(file_name=dataset['en'], encoding=\"UTF-8\")\n",
        "for doc_sv, doc_en in zip(corp_sv.documents[0:2], corp_en.documents[0:2]):\n",
        "  print(doc_sv)\n",
        "  print(doc_en)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmziEm6uwFl",
        "outputId": "a533a7c5-d8f3-4724-db26-639114529d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december', '.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester', '.']\n",
            "['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', ',', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
            "\n",
            "['som', 'ni', 'kunnat', 'konstatera', 'ägde', '&quot;', 'den', 'stora', 'år', '2000-buggen', '&quot;', 'aldrig', 'rum', '.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga', '.']\n",
            "['although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', '&apos;', 'millennium', 'bug', '&apos;', 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Warmup\n"
      ],
      "metadata": {
        "id": "OXrP_ro31vyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common words\n",
        "print([word for word, _ in corp_sv.word_counts.most_common()[0:10]])\n",
        "print([word for word, _ in corp_en.word_counts.most_common()[0:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UaxEQ9315CS",
        "outputId": "c5276d49-ea79-4900-e54a-4eb2a2b165f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'att', ',', 'och', 'i', 'det', 'som', 'för', 'av', 'är']\n",
            "['the', ',', '.', 'of', 'to', 'and', 'in', 'is', 'that', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts and probability of speaker and zebra\n",
        "n_words_en = sum(corp_en.word_counts.values())\n",
        "speaker_prob = corp_en.word_counts['speaker']/n_words_en\n",
        "zebra_prob = corp_en.word_counts['zebra']/n_words_en\n",
        "print(f\"'speaker' - count: {corp_en.word_counts['speaker']}, prob: {speaker_prob:.7f}\")\n",
        "print(f\"'zebra' - count: {corp_en.word_counts['zebra']},  prob: {zebra_prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIPQ-ysh3h-v",
        "outputId": "e41e2267-ab9d-4494-9407-d2d1f4c44a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'speaker' - count: 10, prob: 0.0000355\n",
            "'zebra' - count: 0,  prob: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Language modeling\n",
        "-  *Implement a bigram language model as described in the lecture, and use it to compute the probability of a short sentence.*\n",
        "-  *What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? And what happens if your sentence is very long (e.g. 100 words or more)? Optionally, change your code so that it can handle these challenges.*\n",
        "\n",
        "If we have not seen a pair of words that pair will have a probability of 0. Since we are multiplying the probabilities the full sentence in that case will get a probability of 0 as well. The same is true if there is an unseen word in the sentence.\n",
        "\n",
        "For long sequences the probability goes to zero.\n",
        "Therefore, in order to prevent underflow, we add together the log probabilities instead of multiplying the probabilities.\n",
        "\n",
        "We handle unseen words simply by using defaultdicts that return probability 0 for unseen bigrams.\n"
      ],
      "metadata": {
        "id": "YnQb4GLA4Yg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram language model\n",
        "class Bigram:\n",
        "  def __init__(self, corpus: Corpus):\n",
        "    self.bi_probs = self.__calculate_probs(corpus)\n",
        "\n",
        "  \"\"\"\n",
        "  Caluclates probabilities of bigrams\n",
        "  \"\"\"\n",
        "  def __calculate_probs(self, corpus: Corpus): \n",
        "    # Count occurences of words coming after a word\n",
        "    bigram_counter = defaultdict(Counter)\n",
        "    for words in corpus.documents:\n",
        "      for w1, w2 in zip(words, words[1:]):\n",
        "          bigram_counter[w1][w2] += 1\n",
        "\n",
        "    # Convert counts to probabilities with a default MIN_PROB\n",
        "    bigram_probs = defaultdict(lambda: defaultdict(lambda: MIN_PROB))\n",
        "    for w1, counter in bigram_counter.items():\n",
        "      w1_count = sum(counter.values())\n",
        "      for w2, w2_count in counter.items():\n",
        "        bigram_probs[w1][w2] = w2_count/w1_count\n",
        "\n",
        "    return bigram_probs\n",
        "\n",
        "  \"\"\"Calculate log probability of given sentence (list of words)\"\"\"\n",
        "  def language_modeling(self, words: list):\n",
        "    log_probs = 0.0\n",
        "    for w1, w2 in zip(words, words[1:]):\n",
        "      log_prob = np.log(self.bi_probs[w1][w2])\n",
        "      log_probs += log_prob\n",
        "    return log_probs\n",
        "\n",
        "bigram = Bigram(corp_en)\n",
        "print(bigram.bi_probs[\"speaker\"].items())\n",
        "print(bigram.bi_probs[\"speaker\"][\"said\"])\n",
        "print(bigram.bi_probs[\"speaker\"][\"nonWord\"])\n",
        "print(bigram.bi_probs[\"nonWord\"][\"speaker\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8T1T1PM6qAx",
        "outputId": "bc2ca6ec-1827-47ce-ca2b-eca2ff0958da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('in', 0.1), ('has', 0.1), (',', 0.2), ('already', 0.1), ('off', 0.3), ('had', 0.1), ('said', 0.1)])\n",
            "0.1\n",
            "2.2250738585072014e-308\n",
            "2.2250738585072014e-308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_short = \"i would like\".split()\n",
        "sentence_long = (\"i would like \"*50).split()\n",
        "log_prob_short = bigram.language_modeling(sentence_short)\n",
        "prob_short = np.exp(log_prob_short)\n",
        "log_prob_long = bigram.language_modeling(sentence_long)\n",
        "prob_long = np.exp(log_prob_long)\n",
        "print(f\"Short sentence log prob: {log_prob_short}, prob: {prob_short}\\n\")\n",
        "print(f\"Long sentence log prob: {log_prob_long}, prob: {prob_long}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ACSRj1b61A",
        "outputId": "7f9f7524-6a42-4000-d15e-6a94a947f921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short sentence log prob: -2.956367550031714, prob: 0.052007489078427296\n",
            "\n",
            "Long sentence log prob: -34859.24288558249, prob: 0.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Translation modeling\n",
        "\n",
        "Let's denote target language we wish to translate to as $t$ and source language we wish to translate from as $s$.\n",
        "\n",
        "-  *If our goal is to translate from some language (s) into English (t), why does our conditional probability seem to be written backwards? Why don't we estimate $p(t|s)$ instead?*\n",
        "\n",
        "From Bayes rule we know that \n",
        "\\begin{equation}\n",
        "p(t|s) \\propto_t p(s|t)p(t)\n",
        "\\end{equation}\n",
        "so our objective can be written as\n",
        "\\begin{equation}\n",
        "t^* = \\arg \\max_{t \\in T}  p(t|s) = \\arg \\max_{t \\in T} p(s|t)p(t)\n",
        "\\end{equation}\n",
        "\n",
        "which is suitable since we can use a language model $p(t)$ trained from arbitrary target language text to consider grammar and fluency, while at the same time $p(s|t)$ will handle the translation probabilites.\n"
      ],
      "metadata": {
        "id": "iV7kv_wKliAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import kaiser\n",
        "class TranslationModel:\n",
        "\n",
        "  def __init__(self, src_corpus: Corpus, trg_corpus: Corpus):\n",
        "    self.src_sentences = src_corpus.documents\n",
        "    self.src_vocab = src_corpus.get_vocab()\n",
        "    self.trg_sentences = trg_corpus.documents\n",
        "    self.trg_vocab = trg_corpus.get_vocab()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Calculates translation probs with EM algorithm.\n",
        "  Input a target word to print 10 most aligned words every iteration.\n",
        "  \"\"\"\n",
        "  def calculate_translation_probs(self, trg_word_test=None):\n",
        "    # Represent t(s|t) as nested dicts so that we can index like t[s][t]  \n",
        "    # Use uniform initial translation probs\n",
        "    initial_prob = 1/len(self.src_vocab) \n",
        "    self.trans_probs = defaultdict(lambda: defaultdict(lambda: max(initial_prob, MIN_PROB)))\n",
        "\n",
        "    # Initialization of EM algorithm\n",
        "    n_iters=5\n",
        "    for i in range(1, n_iters+1):\n",
        "      if i % 1 == 0:\n",
        "        print(f\"[{i}/{n_iters}]\")\n",
        "      self.__em_iteration(trg_word_test)\n",
        "    \n",
        "    \n",
        "  \"\"\"\n",
        "  Performs one iteration of the EM algoritm and updates 'trans_probs'\n",
        "  \"\"\"\n",
        "  def __em_iteration(self, trg_word_test):\n",
        "    # Reset counts\n",
        "    t_softcount = defaultdict(lambda: defaultdict(lambda: MIN_PROB))\n",
        "    s_softcount = defaultdict(lambda: MIN_PROB)\n",
        "\n",
        "    for src_sentence, trg_sentence in zip(self.src_sentences, self.trg_sentences): # For each sentence pair\n",
        "      trg_sentence = [\"NULL\"] + trg_sentence # Add NULL word to target sentence \n",
        "\n",
        "      total_count = defaultdict(lambda: MIN_PROB)\n",
        "      for t in trg_sentence: \n",
        "        for s in src_sentence:\n",
        "          total_count[t] += self.trans_probs[s][t]\n",
        "      \n",
        "      for t in trg_sentence: # For each target language word \n",
        "        for s in src_sentence: # For each source language word \n",
        "\n",
        "          # Compute alignment prob \n",
        "          align_prob = self.trans_probs[s][t]/total_count[t]\n",
        "\n",
        "          # Update pseudocount\n",
        "          t_softcount[s][t] += align_prob\n",
        "          s_softcount[s] += align_prob\n",
        "\n",
        "    # Re-estimate probabilities\n",
        "    for s, s_dict in t_softcount.items():\n",
        "      for t, prob in s_dict.items():\n",
        "        self.trans_probs[s][t] = t_softcount[s][t] / s_softcount[s]\n",
        "    \n",
        "    # Print 10 most likely word for a target word\n",
        "    if trg_word_test:\n",
        "      self.print_10_most_likely_src_words(trg_word=trg_word_test)\n",
        "      print(\"-\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Get the top-k most likely target words given a source word\n",
        "  \"\"\"\n",
        "  def get_top_k_likely_trg_words(self, src_word, k=10) -> list:\n",
        "    trg_probs = sorted(self.trans_probs[src_word].items(), key=lambda item: item[1])\n",
        "    trg_probs.reverse()\n",
        "    return trg_probs[:k]\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Get the top-k most likely source words given a target word\n",
        "  \"\"\"\n",
        "  def get_top_k_likely_src_words(self, trg_word, k=10):\n",
        "    src_probs = {s: self.trans_probs[s][trg_word] for s in self.src_vocab }\n",
        "    src_probs_sorted = sorted(src_probs.items(), key=lambda item: item[1])\n",
        "    src_probs_sorted.reverse()\n",
        "    return src_probs_sorted[:k]\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Finds and prints the 10 most likely source words given a target word\n",
        "  \"\"\"\n",
        "  def print_10_most_likely_src_words(self, trg_word):\n",
        "    print(f\"Most likely words for '{trg_word}'\")\n",
        "    top_10_src = self.get_top_k_likely_src_words(trg_word,k=10)\n",
        "    for word, prob in top_10_src:\n",
        "      print(f\"'{word}' : {prob:.3f}\")  \n",
        "    "
      ],
      "metadata": {
        "id": "BtX7ce6HCBcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_model = TranslationModel(corp_sv, corp_en)\n",
        "trans_model.calculate_translation_probs(trg_word_test=\"european\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfpp5ZfAF-UQ",
        "outputId": "848d2212-e441-4e75-e5e8-646b36b427d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/5]\n",
            "Most likely words for 'european'\n",
            "'lugnas' : 0.125\n",
            "'flygsäkerheten' : 0.100\n",
            "'sovjetunionen' : 0.100\n",
            "'hörd' : 0.100\n",
            "'skadat' : 0.091\n",
            "'enhetsakten' : 0.091\n",
            "'kärnpunkt' : 0.091\n",
            "'attraktivt' : 0.083\n",
            "'skikt' : 0.077\n",
            "'europagrupp' : 0.077\n",
            "------------------------------\n",
            "\n",
            "[2/5]\n",
            "Most likely words for 'european'\n",
            "'europeisk' : 0.225\n",
            "'europeiska' : 0.211\n",
            "'europaparlamentet' : 0.175\n",
            "'lugnas' : 0.151\n",
            "'europeiskt' : 0.118\n",
            "'europaparlamentets' : 0.115\n",
            "'sovjetunionen' : 0.114\n",
            "'hörd' : 0.107\n",
            "'csu' : 0.105\n",
            "'unionen' : 0.100\n",
            "------------------------------\n",
            "\n",
            "[3/5]\n",
            "Most likely words for 'european'\n",
            "'europeisk' : 0.526\n",
            "'europeiska' : 0.473\n",
            "'europaparlamentet' : 0.326\n",
            "'europeiskt' : 0.311\n",
            "'europaparlamentets' : 0.244\n",
            "'lugnas' : 0.205\n",
            "'valdeltagandet' : 0.147\n",
            "'unionen' : 0.135\n",
            "'csu' : 0.127\n",
            "'sa' : 0.120\n",
            "------------------------------\n",
            "\n",
            "[4/5]\n",
            "Most likely words for 'european'\n",
            "'europeisk' : 0.712\n",
            "'europeiska' : 0.655\n",
            "'europeiskt' : 0.504\n",
            "'europaparlamentet' : 0.409\n",
            "'europaparlamentets' : 0.334\n",
            "'lugnas' : 0.248\n",
            "'valdeltagandet' : 0.206\n",
            "'bitter' : 0.206\n",
            "'europaval' : 0.173\n",
            "'sa' : 0.162\n",
            "------------------------------\n",
            "\n",
            "[5/5]\n",
            "Most likely words for 'european'\n",
            "'europeisk' : 0.810\n",
            "'europeiska' : 0.764\n",
            "'europeiskt' : 0.626\n",
            "'europaparlamentet' : 0.449\n",
            "'europaparlamentets' : 0.385\n",
            "'bitter' : 0.317\n",
            "'lugnas' : 0.270\n",
            "'europaval' : 0.247\n",
            "'valdeltagandet' : 0.243\n",
            "'europeiske' : 0.220\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding\n",
        "\n",
        "Given a source-language sentence S (foreign language), find target sentence T (english) that has the highest probability according to our model.\n",
        "\n",
        "\\begin{equation}\n",
        "T^* =  \\arg \\max_{T \\in \\mathbf{T}} p(S|T)p(T)\n",
        "\\end{equation}\n",
        "\n",
        "The problem here is to iterate over all possible target sentences T and choose the one wwhich maximizes this objective -> combinatorial problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "OWTah1RD_0Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1) Generate $N$ likely example translations $T_i$.  \n",
        "\n",
        "2) Calculate probability of the example translations with language model $P(T_i)$\n",
        "\n",
        "3) Pick highest probability sentence $P(S|T_i)P(T_i)$\n",
        "\n",
        "Hard part is step 1.\n",
        "\n",
        "Assumptions:\n",
        "- The sentences can only include words already in the corpuses\n",
        "- Markov property: the next word depends only on the current word. Then\n",
        "\\begin{equation}\n",
        " P(S|T) \\approx P(s_1|t_1) \\cdot P(s_2|t_2) \\cdot ... \\cdot P(s_n|t_n)\n",
        "\\end{equation}\n",
        "- $P(T)$ can be calcualted using bigram modeling (markov and independece?)"
      ],
      "metadata": {
        "id": "rAsnt0vMbMAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentence = \"den socialistiska gruppen har begärt ett uttalande från kommissionen om dess strategiska mål för de fem kommande åren .\""
      ],
      "metadata": {
        "id": "bmW_AP7IwEfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an example translation T and estimate P(T|S) = P(S|T)P(T)\n",
        "\n",
        "log_p_st = 0\n",
        "translation=[]\n",
        "for src_word in source_sentence.split():\n",
        "  assert src_word in trans_model.src_vocab, print(f\"Error: '{src_word}' out of vocabulary\")\n",
        "  # Find most likely translated word for this source word\n",
        "  top_k = trans_model.get_top_k_likely_trg_words(src_word, k=1)\n",
        "  t, prob = top_k[0]\n",
        "  log_p_st += np.log(prob)\n",
        "  translation.append(t)\n",
        "\n",
        "#2) Calculate P(T) \n",
        "log_p_t = bigram.language_modeling(translation)\n",
        "\n",
        "#3) Total prob of translated sentence\n",
        "log_p = log_p_st + log_p_t\n",
        "\n",
        "# Get translated string\n",
        "translated_sentence = \" \".join(translation)\n",
        "\n",
        "print(f\"Translated\\n '{source_sentence}'\\n into \\n '{translated_sentence}'\")\n",
        "print(f\"Total log probability: {log_p}\")"
      ],
      "metadata": {
        "id": "9dj0L2879lBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223104ae-6529-46ca-ef30-e0c7a7945103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated\n",
            " 'den socialistiska gruppen har begärt ett uttalande från kommissionen om dess strategiska mål för de fem kommande åren .'\n",
            " into \n",
            " 'the socialist group have requested a statement from commission on its strategic objective for the five next years .'\n",
            "Total log probability: -2206.352595402137\n"
          ]
        }
      ]
    }
  ]
}